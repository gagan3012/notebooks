{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Smart Home Sensor Analysis</h1>\n",
    "<p>The ‘Household Power Consumption‘ dataset is a multivariate time series dataset that describes the electricity consumption for a single household for last few months. The  dataset is modeled after household consumption dataset available here - \n",
    "https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
    "\n",
    "\n",
    "It is a multivariate series comprised of seven variables (besides the date and time); they are:\n",
    "\n",
    "<b>global_active_power:</b> The total active power consumed by the household (kilowatts).<br>\n",
    "<b>global_reactive_power:</b> The total reactive power consumed by the household (kilowatts).<br>\n",
    "<b>voltage:</b> Average voltage (volts).<br>\n",
    "<b>global_intensity:</b> Average current intensity (amps).<br>\n",
    "<b>sub_metering_1:</b> Active energy for kitchen (watt-hours of active energy).<br>\n",
    "<b>sub_metering_2:</b> Active energy for laundry (watt-hours of active energy).<br>\n",
    "<b>sub_metering_3:</b> Active energy for climate control systems (watt-hours of active energy).<br>\n",
    "\n",
    "<p> In the following section, we will analyze and predict hourly power consumption using DeepAR on SageMaker. The purpose of this exercise is to demonstrate Sagemaker integration with IoT Analytics and not to focus on training the right model to generate accurate prediction.\n",
    "<p>For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (1.72.1)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.41.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.17.70)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (0.2.7)\n",
      "Collecting smdebug-rulesconfig==1.0.1\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (3.15.2)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.70 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.20.70)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.70->boto3>=1.16.32->sagemaker) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.70->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.11.1)\n",
      "Requirement already satisfied: dill>=0.3.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.2.9)\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 0.1.4\n",
      "    Uninstalling smdebug-rulesconfig-0.1.4:\n",
      "      Successfully uninstalled smdebug-rulesconfig-0.1.4\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 1.72.1\n",
      "    Uninstalling sagemaker-1.72.1:\n",
      "      Successfully uninstalled sagemaker-1.72.1\n",
      "Successfully installed sagemaker-2.41.0 smdebug-rulesconfig-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()# replace with an existing bucket if needed\n",
    "\n",
    "s3_prefix = 'iot-analytics-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  dataset from the IotAnalytics database and upload it to S3 to make it available for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading from the IotAnalytics database\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "\n",
    "\n",
    "bucket='iotareinvent18'\n",
    "data_key = 'inputdata.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "\n",
    "# Function to convert date columns into a single timestamp field\n",
    "#def parse(x):\n",
    "#    return datetime.strptime(x, '%Y %m %d %H')\n",
    "def parse(x):\n",
    "    t= pd.to_datetime(str(x)) \n",
    "    timestring = t.strftime('%Y.%m.%d %H:%M:%S')\n",
    "    return t\n",
    "\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    " \n",
    "# load sample data\n",
    "\n",
    "#n = sum(1 for line in open(data_location)) - 1 #number of records in file (excludes header)\n",
    "n=244207 #total observations\n",
    "s = 10000 #desired sample size\n",
    "sampling = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "\n",
    "dataset = pd.read_csv(data_location,  header=0, skiprows=sampling ,low_memory=False, infer_datetime_format=True, date_parser = parse)\n",
    "dataset['__dt'] = dataset['timestamp']\n",
    "dataset['cost'] = (dataset['sub_metering_1'] +dataset['sub_metering_2'] + dataset['sub_metering_3'])*1.5\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "dataset.set_index('timestamp', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Examine the dataset\n",
      "dataset shape  (10000, 9)\n",
      "                     global_active_power  global_reactive_power  voltage  \\\n",
      "timestamp                                                                  \n",
      "2018-09-21 11:28:00                 2.05                   0.75   237.85   \n",
      "2018-08-26 00:40:00                 0.29                   0.09   242.73   \n",
      "2018-06-30 05:14:00                 1.54                   0.32   238.85   \n",
      "2018-08-26 20:16:00                 0.38                   0.09   243.18   \n",
      "2018-08-23 07:05:00                 0.43                   0.09   242.36   \n",
      "\n",
      "                     global_intensity  sub_metering_1  sub_metering_2  \\\n",
      "timestamp                                                               \n",
      "2018-09-21 11:28:00               9.8               0               0   \n",
      "2018-08-26 00:40:00               1.4               0               0   \n",
      "2018-06-30 05:14:00               6.6               0               1   \n",
      "2018-08-26 20:16:00               1.6               0               0   \n",
      "2018-08-23 07:05:00               1.8               0               0   \n",
      "\n",
      "                     sub_metering_3                 __dt  cost  \n",
      "timestamp                                                       \n",
      "2018-09-21 11:28:00              11  2018-09-21T11:28:00  16.5  \n",
      "2018-08-26 00:40:00               1   2018-08-26T0:40:00   1.5  \n",
      "2018-06-30 05:14:00              21   2018-06-30T5:14:00  33.0  \n",
      "2018-08-26 20:16:00               1  2018-08-26T20:16:00   1.5  \n",
      "2018-08-23 07:05:00               0   2018-08-23T7:05:00   0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"#####Examine the dataset\")\n",
    "print(\"dataset shape \",dataset.shape)\n",
    "print(dataset.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explore the data</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cor_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"sub_metering_1\",\"sub_metering_2\",\"sub_metering_3\",\"cost\"]\n",
    "\n",
    "\n",
    "# plot correlation matrix\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.matshow(dataset.loc[:, cor_cols].corr(), fignum=1)\n",
    "plt.xticks(range(len(cor_cols)), cor_cols, rotation=90)\n",
    "plt.yticks(range(len(cor_cols)), cor_cols)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Correlation Plot</b>\n",
    "<p>The correlation plot, we can observe that Sub Meters 1 and 2 show some correlation with each other. Sub meter 3 has less correlation with Sub meter 1 and 2. Since cost is derived by aggregating Sub Meter 1, 2 and 3, we will retain only cost column to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##updating data to dataset\n",
    "analysis_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"cost\"]\n",
    "\n",
    "# select the value columns in the DataFrame to compare\n",
    "dataset= dataset.loc[:, analysis_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier. Here we want to forecast longer periods (one week) and resample the data to a granularity of every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = dataset.shape[1]\n",
    "data_kw = dataset.resample('2H').sum()\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 (changed 2 to 1) hour frequency for the time series\n",
    "freq = '2H' \n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12  ##original 7*12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12 ##original 7*12 due to 2H sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assume that the minimum date is the first day sensors started recording the dataset. We split the dataset 70, 30 with 70% of the data retained for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_active_power</th>\n",
       "      <th>global_reactive_power</th>\n",
       "      <th>global_intensity</th>\n",
       "      <th>voltage</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>8.26</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34.8</td>\n",
       "      <td>1446.99</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>42.40</td>\n",
       "      <td>4.73</td>\n",
       "      <td>179.8</td>\n",
       "      <td>11091.73</td>\n",
       "      <td>607.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>63.75</td>\n",
       "      <td>4.59</td>\n",
       "      <td>269.8</td>\n",
       "      <td>17089.58</td>\n",
       "      <td>975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>35.31</td>\n",
       "      <td>4.13</td>\n",
       "      <td>150.8</td>\n",
       "      <td>12753.60</td>\n",
       "      <td>513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>64.33</td>\n",
       "      <td>8.01</td>\n",
       "      <td>277.8</td>\n",
       "      <td>15117.75</td>\n",
       "      <td>1069.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>32.85</td>\n",
       "      <td>4.52</td>\n",
       "      <td>138.4</td>\n",
       "      <td>13795.45</td>\n",
       "      <td>262.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>79.28</td>\n",
       "      <td>4.30</td>\n",
       "      <td>336.0</td>\n",
       "      <td>13942.78</td>\n",
       "      <td>1036.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>61.35</td>\n",
       "      <td>5.11</td>\n",
       "      <td>263.6</td>\n",
       "      <td>14487.99</td>\n",
       "      <td>619.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>54.30</td>\n",
       "      <td>4.70</td>\n",
       "      <td>228.0</td>\n",
       "      <td>12511.39</td>\n",
       "      <td>712.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>48.81</td>\n",
       "      <td>4.66</td>\n",
       "      <td>204.2</td>\n",
       "      <td>12809.79</td>\n",
       "      <td>235.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            global_active_power  global_reactive_power  global_intensity  \\\n",
       "timestamp                                                                  \n",
       "2018-05-30                 8.26                   0.83              34.8   \n",
       "2018-05-31                42.40                   4.73             179.8   \n",
       "2018-06-01                63.75                   4.59             269.8   \n",
       "2018-06-02                35.31                   4.13             150.8   \n",
       "2018-06-03                64.33                   8.01             277.8   \n",
       "...                         ...                    ...               ...   \n",
       "2018-11-21                32.85                   4.52             138.4   \n",
       "2018-11-22                79.28                   4.30             336.0   \n",
       "2018-11-23                61.35                   5.11             263.6   \n",
       "2018-11-24                54.30                   4.70             228.0   \n",
       "2018-11-25                48.81                   4.66             204.2   \n",
       "\n",
       "             voltage    cost  \n",
       "timestamp                     \n",
       "2018-05-30   1446.99   103.5  \n",
       "2018-05-31  11091.73   607.5  \n",
       "2018-06-01  17089.58   975.0  \n",
       "2018-06-02  12753.60   513.0  \n",
       "2018-06-03  15117.75  1069.5  \n",
       "...              ...     ...  \n",
       "2018-11-21  13795.45   262.5  \n",
       "2018-11-22  13942.78  1036.5  \n",
       "2018-11-23  14487.99   619.5  \n",
       "2018-11-24  12511.39   712.5  \n",
       "2018-11-25  12809.79   235.5  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyGroups = dataset.resample('D').sum() ## Gathering unique days for dataset split\n",
    "dailyGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Confirm the datasets\n",
      "startTrainDate, maxDate, traingSplit, splitDate 2018-05-30 00:00:00 2018-11-25 00:00:00 126 2018-10-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "startTrainDate = dailyGroups.index.min()\n",
    "\n",
    "maxDate=dailyGroups.index.max()\n",
    "\n",
    "\n",
    "traingSplit=round(dailyGroups.shape[0]*.7)\n",
    "\n",
    "\n",
    "splitDate=dailyGroups.index[traingSplit]\n",
    "\n",
    "\n",
    "start_dataset = pd.Timestamp(startTrainDate, freq=freq)\n",
    "end_training = pd.Timestamp(splitDate, freq=freq)\n",
    "\n",
    "print(\"###### Confirm the datasets\")\n",
    "print(\"startTrainDate, maxDate, traingSplit, splitDate\", startTrainDate, maxDate, traingSplit, splitDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - pd.Timedelta(1, unit='D')].tolist()\n",
    "         # We use -1 days, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training + pd.Timedelta(k, unit='D') * prediction_length].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 ms, sys: 0 ns, total: 24.1 ms\n",
      "Wall time: 23.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-1-985390189210/iot-analytics-demo-notebook/output/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-1-985390189210/iot-analytics-demo-notebook/output/test/test.json\n",
      "CPU times: user 41.8 ms, sys: 8.36 ms, total: 50.1 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_output_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_output_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2018-05-30 00:00:00\", \"target\": [7.15, 1.1099999999999999, 1.32, 0.42000000000000004, 0.9...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_output_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"40\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"1E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 21:23:40 Starting - Starting the training job...\n",
      "2021-05-18 21:24:03 Starting - Launching requested ML instancesProfilerReport-1621373020: InProgress\n",
      "......\n",
      "2021-05-18 21:25:04 Starting - Preparing the instances for training......\n",
      "2021-05-18 21:26:09 Downloading - Downloading input data...\n",
      "2021-05-18 21:26:33 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:57 INFO 140163793696128] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:57 INFO 140163793696128] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '84', 'time_freq': '2H', 'context_length': '84', 'epochs': '40', 'learning_rate': '1E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:57 INFO 140163793696128] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '1E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '84', 'time_freq': '2H', 'context_length': '84', 'epochs': '40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:57 INFO 140163793696128] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] random_seed is None\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Training set statistics:\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Real time series\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] number of time series: 5\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] number of observations: 7455\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] mean target length: 1491.0\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] min/mean/max target: 0.0/230.08847036483223/3136.090087890625\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] mean abs(target): 230.08847036483223\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] contains missing values: no\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Small number of time series. Doing 128 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Test set statistics:\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Real time series\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] number of time series: 20\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] number of observations: 43000\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] mean target length: 2150.0\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] min/mean/max target: 0.0/240.3857703488372/3136.090087890625\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] mean abs(target): 240.3857703488372\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] contains missing values: no\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] #memory_usage::<batchbuffer> = 20.827598571777344 mb\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] nvidia-smi took: 0.025215625762939453 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Create Store: local\u001b[0m\n",
      "\n",
      "2021-05-18 21:27:04 Training - Training image download completed. Training in progress.\u001b[34m#metrics {\"StartTime\": 1621373218.1494832, \"EndTime\": 1621373218.714915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 562.7467632293701, \"count\": 1, \"min\": 562.7467632293701, \"max\": 562.7467632293701}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:58 INFO 140163793696128] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:26:59 INFO 140163793696128] #memory_usage::<model> = 55 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373218.7150004, \"EndTime\": 1621373219.4197867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1270.1780796051025, \"count\": 1, \"min\": 1270.1780796051025, \"max\": 1270.1780796051025}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:00 INFO 140163793696128] Epoch[0] Batch[0] avg_epoch_loss=4.315720\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:00 INFO 140163793696128] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.3157196044921875\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:01 INFO 140163793696128] Epoch[0] Batch[5] avg_epoch_loss=4.309297\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:01 INFO 140163793696128] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.3092968463897705\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:01 INFO 140163793696128] Epoch[0] Batch [5]#011Speed: 281.82 samples/sec#011loss=4.309297\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373219.419859, \"EndTime\": 1621373222.3367436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"update.time\": {\"sum\": 2916.78786277771, \"count\": 1, \"min\": 2916.78786277771, \"max\": 2916.78786277771}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=217.35150575732183 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.384577083587646\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_1162755c-08f4-4ce0-afe5-f7389ac4b0ba-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373222.3368466, \"EndTime\": 1621373222.4252489, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 87.79525756835938, \"count\": 1, \"min\": 87.79525756835938, \"max\": 87.79525756835938}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] Epoch[1] Batch[0] avg_epoch_loss=4.536571\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:02 INFO 140163793696128] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.5365705490112305\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:04 INFO 140163793696128] Epoch[1] Batch[5] avg_epoch_loss=4.441940\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:04 INFO 140163793696128] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.441940148671468\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:04 INFO 140163793696128] Epoch[1] Batch [5]#011Speed: 304.40 samples/sec#011loss=4.441940\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] Epoch[1] Batch[10] avg_epoch_loss=4.368326\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.279989433288574\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] Epoch[1] Batch [10]#011Speed: 269.03 samples/sec#011loss=4.279989\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373222.4253397, \"EndTime\": 1621373225.2313643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2805.9568405151367, \"count\": 1, \"min\": 2805.9568405151367, \"max\": 2805.9568405151367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=231.28466927165002 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.368326187133789\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_42407ef4-682d-4d53-8aff-0be03d975819-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373225.2314389, \"EndTime\": 1621373225.288151, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 56.17642402648926, \"count\": 1, \"min\": 56.17642402648926, \"max\": 56.17642402648926}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] Epoch[2] Batch[0] avg_epoch_loss=3.668658\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:05 INFO 140163793696128] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.6686575412750244\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:07 INFO 140163793696128] Epoch[2] Batch[5] avg_epoch_loss=4.179683\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:07 INFO 140163793696128] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.179682612419128\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:07 INFO 140163793696128] Epoch[2] Batch [5]#011Speed: 276.69 samples/sec#011loss=4.179683\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] Epoch[2] Batch[10] avg_epoch_loss=4.509305\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=4.904852962493896\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] Epoch[2] Batch [10]#011Speed: 316.91 samples/sec#011loss=4.904853\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373225.288276, \"EndTime\": 1621373228.015883, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2727.5326251983643, \"count\": 1, \"min\": 2727.5326251983643, \"max\": 2727.5326251983643}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=236.83300012053525 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.50930549881675\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] Epoch[3] Batch[0] avg_epoch_loss=4.308576\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:08 INFO 140163793696128] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.308575630187988\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:09 INFO 140163793696128] Epoch[3] Batch[5] avg_epoch_loss=4.221824\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:09 INFO 140163793696128] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.221824328104655\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:09 INFO 140163793696128] Epoch[3] Batch [5]#011Speed: 314.84 samples/sec#011loss=4.221824\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373228.015972, \"EndTime\": 1621373230.320095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2303.575038909912, \"count\": 1, \"min\": 2303.575038909912, \"max\": 2303.575038909912}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=273.4708761099745 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.231780290603638\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_b64d906a-1f7e-4769-a073-2840f8441e4a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373230.3201935, \"EndTime\": 1621373230.3817482, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 60.98794937133789, \"count\": 1, \"min\": 60.98794937133789, \"max\": 60.98794937133789}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] Epoch[4] Batch[0] avg_epoch_loss=4.845019\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:10 INFO 140163793696128] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.845019340515137\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:11 INFO 140163793696128] Epoch[4] Batch[5] avg_epoch_loss=4.380616\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:11 INFO 140163793696128] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.380616386731465\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:11 INFO 140163793696128] Epoch[4] Batch [5]#011Speed: 320.63 samples/sec#011loss=4.380616\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] Epoch[4] Batch[10] avg_epoch_loss=4.063683\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=3.683362627029419\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] Epoch[4] Batch [10]#011Speed: 316.42 samples/sec#011loss=3.683363\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373230.3818302, \"EndTime\": 1621373232.9053924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2523.4897136688232, \"count\": 1, \"min\": 2523.4897136688232, \"max\": 2523.4897136688232}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=258.35903518139025 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.063682859594172\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:12 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_9167827d-332e-4ea9-bf40-bf9f49798260-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373232.9054825, \"EndTime\": 1621373232.958678, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 52.674293518066406, \"count\": 1, \"min\": 52.674293518066406, \"max\": 52.674293518066406}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:13 INFO 140163793696128] Epoch[5] Batch[0] avg_epoch_loss=3.663571\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:13 INFO 140163793696128] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.6635708808898926\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:14 INFO 140163793696128] Epoch[5] Batch[5] avg_epoch_loss=4.146616\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:14 INFO 140163793696128] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.146616498629252\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:14 INFO 140163793696128] Epoch[5] Batch [5]#011Speed: 324.03 samples/sec#011loss=4.146616\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] Epoch[5] Batch[10] avg_epoch_loss=4.229760\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.329531478881836\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] Epoch[5] Batch [10]#011Speed: 308.67 samples/sec#011loss=4.329531\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373232.9587724, \"EndTime\": 1621373235.5053315, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2546.485185623169, \"count\": 1, \"min\": 2546.485185623169, \"max\": 2546.485185623169}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=262.70086496881 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.229759671471336\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:15 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:16 INFO 140163793696128] Epoch[6] Batch[0] avg_epoch_loss=3.834966\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:16 INFO 140163793696128] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.834965705871582\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] Epoch[6] Batch[5] avg_epoch_loss=3.941434\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.941433628400167\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] Epoch[6] Batch [5]#011Speed: 310.58 samples/sec#011loss=3.941434\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373235.5054286, \"EndTime\": 1621373237.832478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2326.554536819458, \"count\": 1, \"min\": 2326.554536819458, \"max\": 2326.554536819458}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=269.4805212318378 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.023799109458923\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:17 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_13be9cb1-ecf2-4d6c-848c-03abf613cba2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373237.8325806, \"EndTime\": 1621373237.8865347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 53.350210189819336, \"count\": 1, \"min\": 53.350210189819336, \"max\": 53.350210189819336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:18 INFO 140163793696128] Epoch[7] Batch[0] avg_epoch_loss=4.105917\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:18 INFO 140163793696128] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.105916976928711\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:19 INFO 140163793696128] Epoch[7] Batch[5] avg_epoch_loss=4.085903\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:19 INFO 140163793696128] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.085903127988179\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:19 INFO 140163793696128] Epoch[7] Batch [5]#011Speed: 327.13 samples/sec#011loss=4.085903\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373237.8866115, \"EndTime\": 1621373240.1858335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2299.152374267578, \"count\": 1, \"min\": 2299.152374267578, \"max\": 2299.152374267578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=275.7367279621474 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.038868069648743\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] Epoch[8] Batch[0] avg_epoch_loss=4.155849\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:20 INFO 140163793696128] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.155849456787109\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:21 INFO 140163793696128] Epoch[8] Batch[5] avg_epoch_loss=4.022863\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:21 INFO 140163793696128] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.022863427797954\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:21 INFO 140163793696128] Epoch[8] Batch [5]#011Speed: 331.28 samples/sec#011loss=4.022863\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] Epoch[8] Batch[10] avg_epoch_loss=4.299173\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=4.630743408203125\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] Epoch[8] Batch [10]#011Speed: 327.59 samples/sec#011loss=4.630743\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373240.185935, \"EndTime\": 1621373242.686715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2500.246047973633, \"count\": 1, \"min\": 2500.246047973633, \"max\": 2500.246047973633}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=259.96072980918717 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.299172509800304\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:22 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:23 INFO 140163793696128] Epoch[9] Batch[0] avg_epoch_loss=3.680392\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:23 INFO 140163793696128] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.680392265319824\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:24 INFO 140163793696128] Epoch[9] Batch[5] avg_epoch_loss=3.990635\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:24 INFO 140163793696128] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.9906347195307412\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:24 INFO 140163793696128] Epoch[9] Batch [5]#011Speed: 333.08 samples/sec#011loss=3.990635\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] Epoch[9] Batch[10] avg_epoch_loss=3.800362\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.57203369140625\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] Epoch[9] Batch [10]#011Speed: 315.86 samples/sec#011loss=3.572034\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373242.6868074, \"EndTime\": 1621373245.1680694, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2480.7424545288086, \"count\": 1, \"min\": 2480.7424545288086, \"max\": 2480.7424545288086}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=262.40807079474564 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.8003615249286997\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_4b69ba5d-a39f-4cab-ba30-3439ebad97b5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373245.1681528, \"EndTime\": 1621373245.220711, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 51.98383331298828, \"count\": 1, \"min\": 51.98383331298828, \"max\": 51.98383331298828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] Epoch[10] Batch[0] avg_epoch_loss=3.748153\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:25 INFO 140163793696128] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.7481529712677\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:26 INFO 140163793696128] Epoch[10] Batch[5] avg_epoch_loss=4.058025\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:26 INFO 140163793696128] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.058024525642395\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:26 INFO 140163793696128] Epoch[10] Batch [5]#011Speed: 324.29 samples/sec#011loss=4.058025\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:27 INFO 140163793696128] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373245.2207747, \"EndTime\": 1621373247.5220294, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2301.1908531188965, \"count\": 1, \"min\": 2301.1908531188965, \"max\": 2301.1908531188965}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:27 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=272.8843397970997 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:27 INFO 140163793696128] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:27 INFO 140163793696128] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.9374994039535522\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:27 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:28 INFO 140163793696128] Epoch[11] Batch[0] avg_epoch_loss=4.283723\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:28 INFO 140163793696128] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=4.283722877502441\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] Epoch[11] Batch[5] avg_epoch_loss=3.977191\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.977191209793091\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] Epoch[11] Batch [5]#011Speed: 331.50 samples/sec#011loss=3.977191\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373247.5221403, \"EndTime\": 1621373249.8263583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2303.673267364502, \"count\": 1, \"min\": 2303.673267364502, \"max\": 2303.673267364502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=271.28859599375755 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.154577922821045\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:29 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:30 INFO 140163793696128] Epoch[12] Batch[0] avg_epoch_loss=4.330471\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:30 INFO 140163793696128] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.330471038818359\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:31 INFO 140163793696128] Epoch[12] Batch[5] avg_epoch_loss=3.936306\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:31 INFO 140163793696128] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.9363059600194297\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:31 INFO 140163793696128] Epoch[12] Batch [5]#011Speed: 322.62 samples/sec#011loss=3.936306\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] Epoch[12] Batch[10] avg_epoch_loss=3.779583\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=3.5915159702301027\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] Epoch[12] Batch [10]#011Speed: 309.95 samples/sec#011loss=3.591516\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373249.8264623, \"EndTime\": 1621373252.3533087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2526.2582302093506, \"count\": 1, \"min\": 2526.2582302093506, \"max\": 2526.2582302093506}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=256.8881583299619 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.7795832373879175\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:32 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_47ff4e70-5865-4fa0-ac4d-f27a49cb44e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373252.3534005, \"EndTime\": 1621373252.4469614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 92.97871589660645, \"count\": 1, \"min\": 92.97871589660645, \"max\": 92.97871589660645}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:33 INFO 140163793696128] Epoch[13] Batch[0] avg_epoch_loss=4.251081\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:33 INFO 140163793696128] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=4.251080513000488\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:33 INFO 140163793696128] Epoch[13] Batch[5] avg_epoch_loss=4.173379\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:33 INFO 140163793696128] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=4.173379262288411\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:33 INFO 140163793696128] Epoch[13] Batch [5]#011Speed: 331.52 samples/sec#011loss=4.173379\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:34 INFO 140163793696128] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373252.4470544, \"EndTime\": 1621373254.7482865, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2301.154851913452, \"count\": 1, \"min\": 2301.154851913452, \"max\": 2301.154851913452}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:34 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=269.84847823050967 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:34 INFO 140163793696128] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:34 INFO 140163793696128] #quality_metric: host=algo-1, epoch=13, train loss <loss>=4.096886205673218\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:34 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:35 INFO 140163793696128] Epoch[14] Batch[0] avg_epoch_loss=3.674647\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:35 INFO 140163793696128] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.674647331237793\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:36 INFO 140163793696128] Epoch[14] Batch[5] avg_epoch_loss=4.144256\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:36 INFO 140163793696128] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=4.144256234169006\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:36 INFO 140163793696128] Epoch[14] Batch [5]#011Speed: 328.15 samples/sec#011loss=4.144256\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373254.748383, \"EndTime\": 1621373257.0698485, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2320.9691047668457, \"count\": 1, \"min\": 2320.9691047668457, \"max\": 2320.9691047668457}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=250.30975543115403 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] #quality_metric: host=algo-1, epoch=14, train loss <loss>=4.111412429809571\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] Epoch[15] Batch[0] avg_epoch_loss=4.725808\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:37 INFO 140163793696128] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=4.7258076667785645\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:38 INFO 140163793696128] Epoch[15] Batch[5] avg_epoch_loss=4.123789\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:38 INFO 140163793696128] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=4.123789072036743\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:38 INFO 140163793696128] Epoch[15] Batch [5]#011Speed: 328.10 samples/sec#011loss=4.123789\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] Epoch[15] Batch[10] avg_epoch_loss=4.286852\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=4.4825273036956785\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] Epoch[15] Batch [10]#011Speed: 329.55 samples/sec#011loss=4.482527\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373257.0699642, \"EndTime\": 1621373259.5552146, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2484.7123622894287, \"count\": 1, \"min\": 2484.7123622894287, \"max\": 2484.7123622894287}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=261.1835518175891 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] #quality_metric: host=algo-1, epoch=15, train loss <loss>=4.286851904608986\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:39 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:40 INFO 140163793696128] Epoch[16] Batch[0] avg_epoch_loss=4.124348\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:40 INFO 140163793696128] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=4.124348163604736\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] Epoch[16] Batch[5] avg_epoch_loss=3.968709\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.968709150950114\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] Epoch[16] Batch [5]#011Speed: 327.49 samples/sec#011loss=3.968709\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373259.5553062, \"EndTime\": 1621373261.832999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2277.1811485290527, \"count\": 1, \"min\": 2277.1811485290527, \"max\": 2277.1811485290527}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=279.2750955292049 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.9757476329803465\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:41 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:42 INFO 140163793696128] Epoch[17] Batch[0] avg_epoch_loss=3.827181\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:42 INFO 140163793696128] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.827180862426758\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:43 INFO 140163793696128] Epoch[17] Batch[5] avg_epoch_loss=4.048958\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:43 INFO 140163793696128] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.04895834128062\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:43 INFO 140163793696128] Epoch[17] Batch [5]#011Speed: 331.38 samples/sec#011loss=4.048958\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] Epoch[17] Batch[10] avg_epoch_loss=4.022207\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=3.990106296539307\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] Epoch[17] Batch [10]#011Speed: 324.21 samples/sec#011loss=3.990106\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373261.833083, \"EndTime\": 1621373264.2812834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2447.669506072998, \"count\": 1, \"min\": 2447.669506072998, \"max\": 2447.669506072998}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=274.1235248602684 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] #quality_metric: host=algo-1, epoch=17, train loss <loss>=4.02220741185275\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] Epoch[18] Batch[0] avg_epoch_loss=3.863258\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:44 INFO 140163793696128] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.863257884979248\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:45 INFO 140163793696128] Epoch[18] Batch[5] avg_epoch_loss=3.949502\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:45 INFO 140163793696128] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.949502428372701\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:45 INFO 140163793696128] Epoch[18] Batch [5]#011Speed: 327.76 samples/sec#011loss=3.949502\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:46 INFO 140163793696128] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373264.2813754, \"EndTime\": 1621373266.5577934, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2275.906562805176, \"count\": 1, \"min\": 2275.906562805176, \"max\": 2275.906562805176}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:46 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=278.1134453834328 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:46 INFO 140163793696128] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:46 INFO 140163793696128] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.964271831512451\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:46 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:47 INFO 140163793696128] Epoch[19] Batch[0] avg_epoch_loss=4.118968\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:47 INFO 140163793696128] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=4.1189680099487305\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:48 INFO 140163793696128] Epoch[19] Batch[5] avg_epoch_loss=3.996943\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:48 INFO 140163793696128] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.9969429969787598\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:48 INFO 140163793696128] Epoch[19] Batch [5]#011Speed: 330.75 samples/sec#011loss=3.996943\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] Epoch[19] Batch[10] avg_epoch_loss=3.819832\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=3.607298803329468\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] Epoch[19] Batch [10]#011Speed: 316.52 samples/sec#011loss=3.607299\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373266.5578942, \"EndTime\": 1621373269.0579545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2499.5298385620117, \"count\": 1, \"min\": 2499.5298385620117, \"max\": 2499.5298385620117}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=256.83483001024956 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.8198319998654453\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] Epoch[20] Batch[0] avg_epoch_loss=3.973527\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:49 INFO 140163793696128] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.9735267162323\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:50 INFO 140163793696128] Epoch[20] Batch[5] avg_epoch_loss=4.000542\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:50 INFO 140163793696128] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=4.000542322794597\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:50 INFO 140163793696128] Epoch[20] Batch [5]#011Speed: 328.71 samples/sec#011loss=4.000542\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373269.0580454, \"EndTime\": 1621373271.317677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2259.1207027435303, \"count\": 1, \"min\": 2259.1207027435303, \"max\": 2259.1207027435303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=273.9850565166416 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] #quality_metric: host=algo-1, epoch=20, train loss <loss>=4.0428160429000854\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] Epoch[21] Batch[0] avg_epoch_loss=4.266971\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:51 INFO 140163793696128] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=4.266970634460449\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:52 INFO 140163793696128] Epoch[21] Batch[5] avg_epoch_loss=4.214613\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:52 INFO 140163793696128] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=4.214612762133281\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:52 INFO 140163793696128] Epoch[21] Batch [5]#011Speed: 330.51 samples/sec#011loss=4.214613\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] Epoch[21] Batch[10] avg_epoch_loss=4.354570\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=4.522518110275269\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] Epoch[21] Batch [10]#011Speed: 323.06 samples/sec#011loss=4.522518\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373271.3177688, \"EndTime\": 1621373273.7938979, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2475.623846054077, \"count\": 1, \"min\": 2475.623846054077, \"max\": 2475.623846054077}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=272.6441051191012 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] #quality_metric: host=algo-1, epoch=21, train loss <loss>=4.354569738561457\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:53 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:54 INFO 140163793696128] Epoch[22] Batch[0] avg_epoch_loss=3.970047\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:54 INFO 140163793696128] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.9700467586517334\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:55 INFO 140163793696128] Epoch[22] Batch[5] avg_epoch_loss=3.832863\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:55 INFO 140163793696128] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.8328634897867837\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:55 INFO 140163793696128] Epoch[22] Batch [5]#011Speed: 321.31 samples/sec#011loss=3.832863\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] Epoch[22] Batch[10] avg_epoch_loss=3.764491\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=3.682443141937256\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] Epoch[22] Batch [10]#011Speed: 328.60 samples/sec#011loss=3.682443\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373273.7939894, \"EndTime\": 1621373276.3232312, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2528.758764266968, \"count\": 1, \"min\": 2528.758764266968, \"max\": 2528.758764266968}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=255.05276693240182 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.7644906044006348\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_f9138f94-1048-477f-ba91-d4d841ec5b34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373276.3233178, \"EndTime\": 1621373276.374171, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 50.3389835357666, \"count\": 1, \"min\": 50.3389835357666, \"max\": 50.3389835357666}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] Epoch[23] Batch[0] avg_epoch_loss=3.974834\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:56 INFO 140163793696128] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.974834442138672\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:57 INFO 140163793696128] Epoch[23] Batch[5] avg_epoch_loss=3.894746\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:57 INFO 140163793696128] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.8947463432947793\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:57 INFO 140163793696128] Epoch[23] Batch [5]#011Speed: 331.30 samples/sec#011loss=3.894746\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] Epoch[23] Batch[10] avg_epoch_loss=4.133868\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=4.42081389427185\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] Epoch[23] Batch [10]#011Speed: 323.46 samples/sec#011loss=4.420814\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373276.374246, \"EndTime\": 1621373278.833051, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2458.740234375, \"count\": 1, \"min\": 2458.740234375, \"max\": 2458.740234375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=275.738347915327 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] #quality_metric: host=algo-1, epoch=23, train loss <loss>=4.133867957375267\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:58 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:59 INFO 140163793696128] Epoch[24] Batch[0] avg_epoch_loss=3.884240\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:27:59 INFO 140163793696128] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.8842403888702393\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:00 INFO 140163793696128] Epoch[24] Batch[5] avg_epoch_loss=3.891887\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:00 INFO 140163793696128] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.891886830329895\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:00 INFO 140163793696128] Epoch[24] Batch [5]#011Speed: 303.90 samples/sec#011loss=3.891887\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373278.8331215, \"EndTime\": 1621373281.1859732, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2352.3313999176025, \"count\": 1, \"min\": 2352.3313999176025, \"max\": 2352.3313999176025}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=257.60331645942176 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] #quality_metric: host=algo-1, epoch=24, train loss <loss>=4.193199849128723\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] Epoch[25] Batch[0] avg_epoch_loss=3.663283\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:01 INFO 140163793696128] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.663283109664917\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:02 INFO 140163793696128] Epoch[25] Batch[5] avg_epoch_loss=3.973053\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:02 INFO 140163793696128] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.9730527798334756\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:02 INFO 140163793696128] Epoch[25] Batch [5]#011Speed: 294.99 samples/sec#011loss=3.973053\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] Epoch[25] Batch[10] avg_epoch_loss=4.049501\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=4.141239500045776\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] Epoch[25] Batch [10]#011Speed: 279.33 samples/sec#011loss=4.141240\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373281.1860616, \"EndTime\": 1621373283.9384162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2751.8160343170166, \"count\": 1, \"min\": 2751.8160343170166, \"max\": 2751.8160343170166}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=246.37137199040137 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] #quality_metric: host=algo-1, epoch=25, train loss <loss>=4.049501289020885\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:03 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:04 INFO 140163793696128] Epoch[26] Batch[0] avg_epoch_loss=3.780033\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:04 INFO 140163793696128] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.7800326347351074\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:05 INFO 140163793696128] Epoch[26] Batch[5] avg_epoch_loss=3.867265\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:05 INFO 140163793696128] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.867265462875366\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:05 INFO 140163793696128] Epoch[26] Batch [5]#011Speed: 279.08 samples/sec#011loss=3.867265\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:06 INFO 140163793696128] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373283.9385014, \"EndTime\": 1621373286.6363993, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2697.3776817321777, \"count\": 1, \"min\": 2697.3776817321777, \"max\": 2697.3776817321777}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:06 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=230.95246547293195 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:06 INFO 140163793696128] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:06 INFO 140163793696128] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.8048511505126954\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:06 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:07 INFO 140163793696128] Epoch[27] Batch[0] avg_epoch_loss=3.932241\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:07 INFO 140163793696128] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.9322407245635986\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] Epoch[27] Batch[5] avg_epoch_loss=4.126934\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=4.126934329668681\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] Epoch[27] Batch [5]#011Speed: 331.02 samples/sec#011loss=4.126934\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373286.6365037, \"EndTime\": 1621373288.907288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2270.240306854248, \"count\": 1, \"min\": 2270.240306854248, \"max\": 2270.240306854248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=273.0840441165506 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] #quality_metric: host=algo-1, epoch=27, train loss <loss>=4.1454520463943485\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:08 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:09 INFO 140163793696128] Epoch[28] Batch[0] avg_epoch_loss=4.113266\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:09 INFO 140163793696128] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.113265514373779\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:10 INFO 140163793696128] Epoch[28] Batch[5] avg_epoch_loss=4.052189\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:10 INFO 140163793696128] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=4.052189191182454\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:10 INFO 140163793696128] Epoch[28] Batch [5]#011Speed: 329.66 samples/sec#011loss=4.052189\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] Epoch[28] Batch[10] avg_epoch_loss=4.124447\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=4.211156749725342\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] Epoch[28] Batch [10]#011Speed: 323.57 samples/sec#011loss=4.211157\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373288.9073708, \"EndTime\": 1621373291.3651128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2457.2155475616455, \"count\": 1, \"min\": 2457.2155475616455, \"max\": 2457.2155475616455}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=261.6664739455358 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] #quality_metric: host=algo-1, epoch=28, train loss <loss>=4.1244471723383125\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] Epoch[29] Batch[0] avg_epoch_loss=3.839128\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:11 INFO 140163793696128] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.839128017425537\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:12 INFO 140163793696128] Epoch[29] Batch[5] avg_epoch_loss=3.957772\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:12 INFO 140163793696128] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.95777161916097\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:12 INFO 140163793696128] Epoch[29] Batch [5]#011Speed: 331.40 samples/sec#011loss=3.957772\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:13 INFO 140163793696128] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373291.3651824, \"EndTime\": 1621373293.6043863, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2238.703489303589, \"count\": 1, \"min\": 2238.703489303589, \"max\": 2238.703489303589}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:13 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=285.4183840619977 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:13 INFO 140163793696128] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:13 INFO 140163793696128] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.878634738922119\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:13 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:14 INFO 140163793696128] Epoch[30] Batch[0] avg_epoch_loss=3.569199\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:14 INFO 140163793696128] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.5691986083984375\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:15 INFO 140163793696128] Epoch[30] Batch[5] avg_epoch_loss=3.891521\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:15 INFO 140163793696128] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.8915205001831055\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:15 INFO 140163793696128] Epoch[30] Batch [5]#011Speed: 334.57 samples/sec#011loss=3.891521\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] Epoch[30] Batch[10] avg_epoch_loss=3.591783\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=3.2320988535881043\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] Epoch[30] Batch [10]#011Speed: 311.69 samples/sec#011loss=3.232099\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373293.6044657, \"EndTime\": 1621373296.0873582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2482.325792312622, \"count\": 1, \"min\": 2482.325792312622, \"max\": 2482.325792312622}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=259.0182827736555 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] #progress_metric: host=algo-1, completed 77.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.5917833880944685\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/state_50f91aa1-39ba-47b8-9801-37c88a1ef48e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373296.0874426, \"EndTime\": 1621373296.1390002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 50.984859466552734, \"count\": 1, \"min\": 50.984859466552734, \"max\": 50.984859466552734}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] Epoch[31] Batch[0] avg_epoch_loss=4.236077\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:16 INFO 140163793696128] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=4.236077308654785\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:17 INFO 140163793696128] Epoch[31] Batch[5] avg_epoch_loss=3.901500\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:17 INFO 140163793696128] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.9015003045399985\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:17 INFO 140163793696128] Epoch[31] Batch [5]#011Speed: 324.76 samples/sec#011loss=3.901500\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373296.1390617, \"EndTime\": 1621373298.3916514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2252.5227069854736, \"count\": 1, \"min\": 2252.5227069854736, \"max\": 2252.5227069854736}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=277.0049668879429 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.8782167434692383\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] Epoch[32] Batch[0] avg_epoch_loss=3.756501\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:18 INFO 140163793696128] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.7565014362335205\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:19 INFO 140163793696128] Epoch[32] Batch[5] avg_epoch_loss=4.093075\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:19 INFO 140163793696128] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=4.093075354894002\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:19 INFO 140163793696128] Epoch[32] Batch [5]#011Speed: 332.38 samples/sec#011loss=4.093075\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:20 INFO 140163793696128] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373298.3917544, \"EndTime\": 1621373300.4797056, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2087.3754024505615, \"count\": 1, \"min\": 2087.3754024505615, \"max\": 2087.3754024505615}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:20 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=270.1770980523298 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:20 INFO 140163793696128] #progress_metric: host=algo-1, completed 82.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:20 INFO 140163793696128] #quality_metric: host=algo-1, epoch=32, train loss <loss>=4.1735680103302\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:20 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:21 INFO 140163793696128] Epoch[33] Batch[0] avg_epoch_loss=3.715309\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:21 INFO 140163793696128] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.7153091430664062\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] Epoch[33] Batch[5] avg_epoch_loss=3.933215\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.9332149823506675\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] Epoch[33] Batch [5]#011Speed: 333.97 samples/sec#011loss=3.933215\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373300.479809, \"EndTime\": 1621373302.77749, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2297.1396446228027, \"count\": 1, \"min\": 2297.1396446228027, \"max\": 2297.1396446228027}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=272.0606656760242 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.901195526123047\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:22 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:23 INFO 140163793696128] Epoch[34] Batch[0] avg_epoch_loss=4.180233\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:23 INFO 140163793696128] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=4.180233001708984\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:24 INFO 140163793696128] Epoch[34] Batch[5] avg_epoch_loss=4.168673\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:24 INFO 140163793696128] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=4.168673157691956\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:24 INFO 140163793696128] Epoch[34] Batch [5]#011Speed: 331.82 samples/sec#011loss=4.168673\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373302.77759, \"EndTime\": 1621373305.0915337, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2313.4090900421143, \"count\": 1, \"min\": 2313.4090900421143, \"max\": 2313.4090900421143}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=269.71506726837265 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] #progress_metric: host=algo-1, completed 87.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.987425422668457\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] Epoch[35] Batch[0] avg_epoch_loss=3.972865\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:25 INFO 140163793696128] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.972864866256714\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:26 INFO 140163793696128] Epoch[35] Batch[5] avg_epoch_loss=3.999356\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:26 INFO 140163793696128] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.9993563890457153\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:26 INFO 140163793696128] Epoch[35] Batch [5]#011Speed: 322.27 samples/sec#011loss=3.999356\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] Epoch[35] Batch[10] avg_epoch_loss=4.229320\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=4.505276441574097\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] Epoch[35] Batch [10]#011Speed: 320.73 samples/sec#011loss=4.505276\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373305.0916348, \"EndTime\": 1621373307.584599, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2492.3410415649414, \"count\": 1, \"min\": 2492.3410415649414, \"max\": 2492.3410415649414}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=259.58009490496113 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] #quality_metric: host=algo-1, epoch=35, train loss <loss>=4.229320049285889\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:27 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:28 INFO 140163793696128] Epoch[36] Batch[0] avg_epoch_loss=3.151853\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:28 INFO 140163793696128] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=3.15185284614563\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] Epoch[36] Batch[5] avg_epoch_loss=3.769867\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.7698673407236734\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] Epoch[36] Batch [5]#011Speed: 334.04 samples/sec#011loss=3.769867\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373307.5847, \"EndTime\": 1621373309.811078, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2225.8315086364746, \"count\": 1, \"min\": 2225.8315086364746, \"max\": 2225.8315086364746}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=282.12340810252664 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] #progress_metric: host=algo-1, completed 92.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.8312776565551756\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:29 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:30 INFO 140163793696128] Epoch[37] Batch[0] avg_epoch_loss=3.934289\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:30 INFO 140163793696128] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.9342892169952393\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:31 INFO 140163793696128] Epoch[37] Batch[5] avg_epoch_loss=3.824078\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:31 INFO 140163793696128] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.824077566464742\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:31 INFO 140163793696128] Epoch[37] Batch [5]#011Speed: 327.18 samples/sec#011loss=3.824078\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373309.8111804, \"EndTime\": 1621373312.056004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2244.295835494995, \"count\": 1, \"min\": 2244.295835494995, \"max\": 2244.295835494995}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=266.43610520906077 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.9892611503601074\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] Epoch[38] Batch[0] avg_epoch_loss=3.478897\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:32 INFO 140163793696128] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=3.4788973331451416\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:33 INFO 140163793696128] Epoch[38] Batch[5] avg_epoch_loss=3.820560\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:33 INFO 140163793696128] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.820560336112976\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:33 INFO 140163793696128] Epoch[38] Batch [5]#011Speed: 307.15 samples/sec#011loss=3.820560\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373312.0561082, \"EndTime\": 1621373314.4108684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2354.1972637176514, \"count\": 1, \"min\": 2354.1972637176514, \"max\": 2354.1972637176514}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=269.29238003237964 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] #progress_metric: host=algo-1, completed 97.5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.888160800933838\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] Epoch[39] Batch[0] avg_epoch_loss=4.264407\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:34 INFO 140163793696128] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=4.264406681060791\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:35 INFO 140163793696128] Epoch[39] Batch[5] avg_epoch_loss=3.979289\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:35 INFO 140163793696128] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.979288856188456\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:35 INFO 140163793696128] Epoch[39] Batch [5]#011Speed: 326.01 samples/sec#011loss=3.979289\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] Epoch[39] Batch[10] avg_epoch_loss=4.207549\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=4.481461095809936\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] Epoch[39] Batch [10]#011Speed: 321.60 samples/sec#011loss=4.481461\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373314.4109538, \"EndTime\": 1621373316.8713388, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2459.8162174224854, \"count\": 1, \"min\": 2459.8162174224854, \"max\": 2459.8162174224854}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] #throughput_metric: host=algo-1, train throughput=270.7382892198163 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] #quality_metric: host=algo-1, epoch=39, train loss <loss>=4.207548965107311\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] Final loss: 3.5917833880944685 (occurred at epoch 30)\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] #quality_metric: host=algo-1, train final_loss <loss>=3.5917833880944685\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 WARNING 140163793696128] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:36 INFO 140163793696128] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373316.8714197, \"EndTime\": 1621373317.640607, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 768.2676315307617, \"count\": 1, \"min\": 768.2676315307617, \"max\": 768.2676315307617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373317.6406858, \"EndTime\": 1621373317.8653872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 993.0872917175293, \"count\": 1, \"min\": 993.0872917175293, \"max\": 993.0872917175293}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373317.8654754, \"EndTime\": 1621373317.8962398, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 30.718088150024414, \"count\": 1, \"min\": 30.718088150024414, \"max\": 30.718088150024414}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] #memory_usage::<batchbuffer> = 20.827598571777344 mb\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:37 INFO 140163793696128] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373317.8962996, \"EndTime\": 1621373317.897301, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.03814697265625, \"count\": 1, \"min\": 0.03814697265625, \"max\": 0.03814697265625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373317.897357, \"EndTime\": 1621373321.2325683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 3335.30855178833, \"count\": 1, \"min\": 3335.30855178833, \"max\": 3335.30855178833}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, RMSE): 212.86524877851045\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, mean_absolute_QuantileLoss): 118384.93187573164\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, mean_wQuantileLoss): 0.29010206401814875\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.1]): 0.15236729082519007\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.2]): 0.2405630467214042\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.3]): 0.315125849566619\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.4]): 0.35885071497734605\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.5]): 0.3735714880170911\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.6]): 0.363484475494486\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.7]): 0.3300675246833181\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.8]): 0.27984154171920933\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #test_score (algo-1, wQuantileLoss[0.9]): 0.19704664415867465\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #quality_metric: host=algo-1, test RMSE <loss>=212.86524877851045\u001b[0m\n",
      "\u001b[34m[05/18/2021 21:28:41 INFO 140163793696128] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.29010206401814875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1621373321.2326453, \"EndTime\": 1621373321.2830045, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 8.115768432617188, \"count\": 1, \"min\": 8.115768432617188, \"max\": 8.115768432617188}, \"totaltime\": {\"sum\": 103500.21767616272, \"count\": 1, \"min\": 103500.21767616272, \"max\": 103500.21767616272}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-05-18 21:29:04 Uploading - Uploading generated training model\n",
      "2021-05-18 21:29:04 Completed - Training job completed\n",
      "ProfilerReport-1621373020: NoIssuesFound\n",
      "Training seconds: 165\n",
      "Billable seconds: 165\n",
      "CPU times: user 836 ms, sys: 76.1 ms, total: 912 ms\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_output_path),\n",
    "    \"test\": \"{}/test/\".format(s3_output_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.1\n",
      "  Using cached sagemaker-1.72.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (1.19.5)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (1.17.70)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (3.7.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (1.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (20.9)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Using cached smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.1) (3.15.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.70 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.1) (1.20.70)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.1) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.1) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.70->boto3>=1.14.12->sagemaker==1.72.1) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.70->boto3>=1.14.12->sagemaker==1.72.1) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.1) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.1) (1.15.0)\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.41.0\n",
      "    Uninstalling sagemaker-2.41.0:\n",
      "      Successfully uninstalled sagemaker-2.41.0\n",
      "Successfully installed sagemaker-1.72.1 smdebug-rulesconfig-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "respDF=predictor.predict(ts=timeseries[4], quantiles=[0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted cost for next 10 hours  at 0.9 quantile. Please note, the data is generated using Device simulators and not recorded by actual sensors. Predicted cost can sometime display negative numbers, which will not be the case in case of real life scenario. Power company will never credit money for low consumption!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-26 00:00:00</th>\n",
       "      <td>61.393833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 02:00:00</th>\n",
       "      <td>36.448086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 04:00:00</th>\n",
       "      <td>41.201164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 06:00:00</th>\n",
       "      <td>60.088306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 08:00:00</th>\n",
       "      <td>77.395706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 10:00:00</th>\n",
       "      <td>86.747620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 12:00:00</th>\n",
       "      <td>39.131981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 14:00:00</th>\n",
       "      <td>76.918724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 16:00:00</th>\n",
       "      <td>93.551407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26 18:00:00</th>\n",
       "      <td>108.740021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0.9\n",
       "2018-11-26 00:00:00   61.393833\n",
       "2018-11-26 02:00:00   36.448086\n",
       "2018-11-26 04:00:00   41.201164\n",
       "2018-11-26 06:00:00   60.088306\n",
       "2018-11-26 08:00:00   77.395706\n",
       "2018-11-26 10:00:00   86.747620\n",
       "2018-11-26 12:00:00   39.131981\n",
       "2018-11-26 14:00:00   76.918724\n",
       "2018-11-26 16:00:00   93.551407\n",
       "2018-11-26 18:00:00  108.740021"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor,\n",
    "    target_ts,\n",
    "    cat=None,\n",
    "    dynamic_feat=None,\n",
    "    forecast_date=end_training,\n",
    "    show_samples=False,\n",
    "    plot_history=7 * 12,\n",
    "    confidence=80,\n",
    "):\n",
    "    freq = target_ts.index.freq\n",
    "    print(\n",
    "        \"calling served model to generate predictions starting from {}\".format(str(forecast_date))\n",
    "    )\n",
    "    assert confidence > 50 and confidence < 100\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "\n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100,\n",
    "    }\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, \"cat = {}\".format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples:\n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color=\"lightskyblue\", alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "    # plot the target\n",
    "    target_section = target_ts[\n",
    "        forecast_date - plot_history * freq : forecast_date + prediction_length * freq\n",
    "    ]\n",
    "    target_section.plot(color=\"black\", label=\"target\")\n",
    "\n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index,\n",
    "        prediction[str(low_quantile)].values,\n",
    "        prediction[str(up_quantile)].values,\n",
    "        color=\"b\",\n",
    "        alpha=0.3,\n",
    "        label=\"{}% confidence interval\".format(confidence),\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label=\"P50\")\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.date_range(\n",
    "                    start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)\n",
    "                ),\n",
    "                data=f,\n",
    "            )\n",
    "            feat_ts[\n",
    "                forecast_date - plot_history * freq : forecast_date + prediction_length * freq\n",
    "            ].plot(ax=ax, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ad8bb82574dcbbdb0bf968208a0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='customer_id', max=5, style=SliderStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style = {\"description_width\": \"initial\"}\n",
    "from datetime import timedelta\n",
    "@interact_manual(\n",
    "    customer_id=IntSlider(min=0, max=5, value=1, style=style),\n",
    "    forecast_day=IntSlider(min=0, max=100, value=51, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False,\n",
    ")\n",
    "def plot_interact(customer_id, forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[customer_id],\n",
    "        forecast_date=end_training +timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * 12 * 7,\n",
    "        confidence=confidence,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
